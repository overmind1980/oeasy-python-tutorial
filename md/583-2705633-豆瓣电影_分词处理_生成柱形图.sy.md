---
show: step
version: 1.0
enable_checker: true
---

# çˆ¬å–å›¾ç‰‡æ•°æ®

## å›å¿†

- ä¸Šæ¬¡çˆ¬äº†æ±‰å­—æº
- å›¾ç‰‡ä¸åªæ˜¯å¯ä»¥å•ç‹¬å­˜åœ¨
- ä¹Ÿå¯ä»¥åµŒå…¥åˆ°ç½‘é¡µä¸­çš„
- åº”è¯¥æ³¨æ„åˆ°è¯·æ±‚å¤´é‡Œé¢å¯ä»¥æ”¾ä¸œè¥¿
- çˆ¬å–çš„æ—¶å€™å¯ä»¥åˆ©ç”¨åŸæ¥ç½‘é¡µæ•´ä½“çš„ç»“æ„

### è§‚å¯Ÿè±†ç“£ç”µå½±250

```
firefox http://movie.douban.com/top250
```

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717202833221)

### åˆ†é¡µçˆ¬

```
# å¯¼åŒ…
import requests
from lxml import etree      # ç”¨lxmlè§£æå™¨ç”Ÿæˆçš„å¯¹è±¡ä¸­çš„xpathæ–¹æ³•
from time import sleep
import csv
import numpy as np


# æŒ‡å®šURL
# url = 'https://movie.douban.com/top250'
# è¿›è¡ŒUAä¼ªè£…
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.41'}

# å®šä¹‰ç©ºåˆ—è¡¨å­˜æ”¾ç”µå½±æ•°æ®
tiltes_cn = []       # ä¸­æ–‡æ ‡é¢˜
titles_en = []       # è‹±æ–‡æ ‡é¢˜
links = []           # è¯¦æƒ…é¡µé“¾æ¥
director = []        # å¯¼æ¼”
actors = []          # æ¼”å‘˜
years = []           # ä¸Šæ˜ å¹´ä»½
nations = []         # å›½å®¶å’Œåœ°åŒº
types = []           # ç±»å‹
scores = []           # è¯„åˆ†
rating_nums = []      # è¯„åˆ†äººæ•°

fp = open('douban_top250.csv','w',encoding='utf-8')
writer = csv.writer(fp)
writer.writerow(['ç”µå½±ä¸­æ–‡å', 'ç”µå½±è‹±æ–‡å','ç”µå½±è¯¦æƒ…é¡µé“¾æ¥','å¯¼æ¼”','æ¼”å‘˜','ä¸Šæ˜ å¹´ä»½','å›½å®¶å’Œåœ°åŒº','ç±»å‹','è¯„åˆ†','è¯„åˆ†äººæ•°'])

for i in range(0,226,25):
    url = f'https://movie.douban.com/top250?start={i}&filter='
    # å°†URLä¸­çš„å‚æ•°å°è£…åˆ°å­—å…¸ä¸­
    # data = {
    #     'start':i,            # è®¾ç½®startå‚æ•°
    #     'filter':'',
    # }
    # å‘èµ·è¯·æ±‚ï¼Œè·å–ç½‘é¡µå“åº”
    response = requests.get(url,headers=headers
                            # ,data=data
                           )
    sleep(1)
    # print(response.status_code)
    # print(response.encoding)
    # print(response.text)
    
    # è·å–å“åº”å†…å®¹
    html = response.text  
    # å®ä¾‹åŒ–ä¸€ä¸ªetreeå¯¹è±¡
    data = etree.HTML(html)
    
    # æ‰€æœ‰ç”µå½±ä¿¡æ¯éƒ½åœ¨liæ ‡ç­¾ä¸‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆå®šä½åˆ°liæ ‡ç­¾ï¼Œåœ¨é€šè¿‡å¾ªç¯è·å–æ¯ä¸€ä¸ªliæ ‡ç­¾ä¸­çš„ä¿¡æ¯
    li_list = data.xpath('//*[@id="content"]/div/div[1]/ol/li')
    # é€šè¿‡å¾ªç¯éå†æ¯ä¸€é¡µä¸­çš„æ‰€æœ‰liæ ‡ç­¾ï¼Œè·å–è¯¥é¡µé¢æ‰€æœ‰ç”µå½±çš„æ•°æ®
    for each in li_list:
        # ä¸­æ–‡æ ‡é¢˜
        title1 = each.xpath('./div/div[2]/div[1]/a/span[1]/text()')[0] 
        tiltes_cn.append(title1)
        
        # è‹±æ–‡æ ‡é¢˜
        # æ¯æ¬¡è·å–åˆ°çš„æ•°æ®å­˜åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œé€šè¿‡ä¸‹æ ‡ç´¢å¼•å–åˆ—è¡¨çš„å€¼
        # é€šè¿‡å­—ç¬¦ä¸²çš„strip()æ–¹æ³•å»é™¤å­—ç¬¦ä¸²é¦–å°¾çš„æŒ‡å®šå­—ç¬¦ä¸²
        title2 = each.xpath('./div/div[2]/div[1]/a/span[2]/text()')[0].strip('\xa0/\xa0')
        titles_en.append(title2)
        
        # é“¾æ¥
        link = each.xpath('./div/div[2]/div[1]/a/@href')[0]     
        links.append(link)
        
        # å¯¼æ¼”ã€ä¸»æ¼”
        info1 = each.xpath('./div/div[2]/div[2]/p[1]/text()[1]')[0].strip()       # é€šè¿‡stripæ–¹æ³•å»é™¤å­—ç¬¦ä¸²çš„å‰åç©ºæ ¼
        split_info1 = info1.split('\xa0\xa0\xa0')      # é€šè¿‡æŒ‡å®šå­—ç¬¦ä¸²åˆ†å‰²å­—ç¬¦ä¸²
        dirt = split_info1[0].strip('å¯¼æ¼”: ')
        director.append(dirt)
        # æœ‰äº›ç”µå½±çš„ä¸»æ¼”ä¸ºç©ºï¼Œæ‰€ä»¥éœ€è¦è¿›è¡Œæ¡ä»¶åˆ¤æ–­
        # å¦‚æœå¯¼æ¼”å’Œä¸»æ¼”ä¿¡æ¯éƒ½æœ‰ï¼Œåˆ™è·å–ä¸»æ¼”ä¿¡æ¯
        if len(split_info1) == 2:
            ac = split_info1[1].strip('ä¸»æ¼”: ')
            actors.append(ac)
        # å¦‚æœæ²¡æœ‰ä¸»æ¼”ä¿¡æ¯ï¼Œåˆ™å°†å…¶ä¿¡æ¯è®¾ç½®ä¸ºç©º
        else:
            actors.append(np.nan)
        
        # å¹´ä»½ã€å›½ç±ã€ç±»å‹
        info2 = each.xpath('./div/div[2]/div[2]/p[1]/text()[2]')[0].strip()    # å»é™¤å­—ç¬¦ä¸²é¦–å°¾çš„ç©ºæ ¼
        split_info2 = info2.split('\xa0/\xa0')    # é€šè¿‡å­—ç¬¦ä¸²åˆ†å‰²è·å–å­—ç¬¦ä¸²ä¸­çš„å¹´ä»½ã€å›½ç±å’Œç±»å‹
        # print(split_info)
        year = split_info2[0]
        nation = split_info2[1]
        ftype = split_info2[2]
        years.append(year)
        nations.append(nation)
        types.append(ftype)
        
        # ç”µå½±è¯„åˆ†
        score = each.xpath('./div/div[2]/div[2]/div/span[2]/text()')[0]
        scores.append(score)
        
        # è·å–ç”µå½±æ‰“åˆ†äººæ•°
        num = each.xpath('./div/div[2]/div[2]/div/span[4]/text()')[0].strip('äººè¯„ä»·')
        rating_nums.append(num)
        
        writer.writerow([title1,title2,link,dirt,ac,year,nation,ftype,score,num])
    print(f'â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ç¬¬{int((i/25)+1)}é¡µçˆ¬å–å®Œæ¯•ï¼â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”')
        
        
    
fp.close()   # å†™å…¥å®Œæˆåï¼Œå…³é—­æ–‡ä»¶
print('â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”çˆ¬è™«ç»“æŸï¼ï¼ï¼ï¼ï¼â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”')
```

### è§‚å¯Ÿç»“æœ

```
sudo apt install csvtool
```

- å®‰è£…åè§‚å¯Ÿå›½åˆ«

```
csvtool col 1,7 douban_top250.csv
```

- æ•ˆæœ

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717202931071)

- è¯„åˆ†è°æœ€é«˜?è°æœ€ä½?

### è¯„åˆ† 

```
import pandas as pd

df = pd.read_csv('douban_top250.csv', sep=",")

score = df['è¯„åˆ†'].max()
print("æœ€é«˜è¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†'] == score]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\tåˆ†æ•°æ˜¯ï¼š", score)
score = df['è¯„åˆ†'].min()
print("æœ€ä½è¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†'] == score]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\tåˆ†æ•°æ˜¯ï¼š", score)
```

- è¯„åˆ†é«˜ä½

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717223980376)

- è¯„åˆ†äººæ•°
	- è°é«˜è°ä½?

### è¯„åˆ†äººæ•°

```
import pandas as pd

df = pd.read_csv('douban_top250.csv', sep=",")

num = df['è¯„åˆ†äººæ•°'].max()
print("æœ€å¤šäººè¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†äººæ•°'] == num]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\täººæ•°æ˜¯ï¼š", num)
num = df['è¯„åˆ†äººæ•°'].min()
print("æœ€å°‘äººè¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†äººæ•°'] == num]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\täººæ•°æ˜¯ï¼š", num)
```

- è¯„åˆ†äººæ•°

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717224016678)

- å„ç§ç±»å‹
	- è°å¤šè°å°‘?


### å®‰è£…ç±»åº“

- å®‰è£…è¯äº‘å’Œåˆ†è¯åŒ…

```
pip3 install wordcloud
pip3 install jieba
```

### ç±»å‹

```
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
import csv

def segment(text):
    for ch in '!"#$%&()*+,-./:;<=>?@[\\]^_â€˜{|}~ï¼Œã€‚ï¼Ÿï¼ï¼šï¼›â€â€œâ€™â€˜-=ã€Šã€‹ï¼ˆï¼‰ã€ã€ã€‘â†“LO':
        text = text.replace(ch, " ")
    words_list_jieba = jieba.lcut(text)
    words_list = []
    for i in words_list_jieba:
        words_list.append(i)
    return words_list

def words_frequency(words_list):
    words_dict = {}
    for key in words_list:
        words_dict[key] = words_dict.get(key, 0) + 1
    return words_dict

def words_dict_sort(words_dict, descending):
    temp_dict = sorted(words_dict.items(), key=lambda x: x[1], reverse=descending)
    return temp_dict

df = pd.read_csv('douban_top250.csv', sep=",")
text = ' '.join(df['ç±»å‹'].dropna())
words_list = segment(text)
words_dict = words_frequency(words_list)
print(words_dict)
```

1. å°†æ ‡ç‚¹æ›¿æ¢ä¸ºç©ºæ ¼
2. å°†å­—ç¬¦ä¸²åˆ†è¯ç”Ÿæˆè¯åˆ—è¡¨
3. å°†è¯åˆ—è¡¨ç”Ÿæˆè¯é¢‘å­—å…¸
4. å°†è¯é¢‘å­—å…¸è¾“å‡º

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717224399325)

### ç»“æœ

- ç»“æœå¯ä»¥ç”Ÿæˆ
	- æŸ±é¥¼æŠ˜
	- è¯äº‘å›¾

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717224451129)

- æƒ³è¦å¾—åˆ° 
	- å…³äºå›½å®¶å’Œåœ°åŒºçš„
	- è¯é¢‘å­—å…¸

### æ•°æ®æ¸…æ´—

- å¦‚æœæƒ³è¦å°†ä¸€äº›è¯æ±‡
	- ä»ç»“æœä¸­æ¸…æ´—å‘¢?

```
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
import csv

def segment(text):
    for ch in '!"#$%&()*+,-./:;<=>?@[\\]^_â€˜{|}~ï¼Œã€‚ï¼Ÿï¼ï¼šï¼›â€â€œâ€™â€˜-=ã€Šã€‹ï¼ˆï¼‰\nã€ã€‘â†“LO':
        text = text.replace(ch, " ")
    words_list_jieba = jieba.lcut(text)
    words_list = []
    for i in words_list_jieba:
        if i in ["ä½ ", "UNG", "å¥¹", "ä¸‰å¨˜", "è¿™ä½", "å¾®åš", "è§†è§’", "0", "ç§€è‹±", "å¾ˆ", "çƒ­è¯", "ä¸¤ä¼š", "çœ‹","è¿™äº›", "èŠ", "æˆ‘ä»¬", "å…³äº", "è¦", "è§†é¢‘", "è¿™", "é‚£", "æœ‰", "ä¸", "åœ¨", "å´", "éƒ½","å°±", "æ˜¯", "å°±æ˜¯", "æˆ‘", "ä»–", "å®ƒ", "ä»–ä»¬","å®ƒä»¬", "çš„", "ç€", "äº†", "å’Œ", "ä¸", "ä¸ª", "ä¸€ä¸ª", "ä¸€","ã€Š", "ã€‹", " ", "\n"] or i.isnumeric():
            pass
        else:
            words_list.append(i)
    return words_list

def words_frequency(words_list):
    words_dict = {}
    for key in words_list:
        words_dict[key] = words_dict.get(key, 0) + 1
    return words_dict

def words_dict_sort(words_dict, descending):
    temp_dict = sorted(words_dict.items(), key=lambda x: x[1], reverse=descending)
    return temp_dict

df = pd.read_csv('douban_top250.csv', sep=",")

text = ' '.join(df['å›½å®¶å’Œåœ°åŒº'].dropna())
words_list = segment(text)
words_dict = words_frequency(words_list)
wordcloud = WordCloud(font_path="/usr/share/fonts/truetype/wqy-zenhei.ttc",  # ä¸ºæ˜¾ç¤ºæ­£ç¡®çš„ä¸­æ–‡ï¼Œéœ€æŒ‡å®šå­—ä½“
                      background_color="white",  # èƒŒæ™¯é¢œè‰²
                      collocations=False,
                      width=800,
                      height=600,
                      margin=2,  # è®¾ç½®å›¾ç‰‡é»˜è®¤çš„å¤§å°
                      ).fit_words(words_dict)
wordcloud.to_file('nation.png')

```

- å¯ä»¥æŠŠæŸäº›å•è¯
	- æ’é™¤

### æœ€ç»ˆç»“æœ

- é¦–å…ˆå£°æ˜
	- é¦™æ¸¯åœ°åŒºã€æ¾³é—¨åœ°åŒºã€å°æ¹¾åœ°åŒºæ˜¯
	- ä¸­å›½ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨åˆ†

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717225094789)

- å¦‚æœæƒ³è¦å°†
	- ä¸­å›½å¤§é™†ã€ä¸­å›½é¦™æ¸¯ã€ä¸­å›½å°æ¹¾
	- ä½œä¸ºåœ°åŒº åˆ†ç±»ç»Ÿè®¡

### æ·»åŠ è¯æ±‡

- å°†ä¸‰ä¸ªå•è¯åŠ å…¥å­—å…¸è¯æ¡

```
jieba.add_word("ä¸­å›½å¤§é™†", freq=None, tag=None)
jieba.add_word("ä¸­å›½å°æ¹¾", freq=None, tag=None)
jieba.add_word("ä¸­å›½é¦™æ¸¯", freq=None, tag=None)
```

- ç»“æœ

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717225450110)

- æœŸå¾…ç¬¬å…«ã€ä¹è‰ºæœ¯èµ·é£

### æ•°æ®åˆ†æ

```
import bs4
import requests

import json

import re
import matplotlib.pyplot as plt
from matplotlib import rcParams
from collections import Counter

rcParams['font.sans-serif'] = ['SimHei']  # ä½¿ç”¨é»‘ä½“å­—ä½“ï¼Œç¡®ä¿ä½ çš„ç³»ç»Ÿä¸­æœ‰è¿™ä¸ªå­—ä½“
rcParams['axes.unicode_minus'] = False  # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36 SLBrowser/9.0.3.5211 SLBChan/105'}

movies = []

for page in range(1, 11):
    offset = (page - 1) * 25  # è®¡ç®—åç§»é‡
    url = f'https://movie.douban.com/top250?start={offset}&filter='

    resp = requests.get(url, headers=headers)

    if resp.status_code != 200:
        print(f"Failed to fetch page {page}. Status code: {resp.status_code}")
        continue

        # åˆ›å»ºBeautifulSoupå¯¹è±¡
    soup = bs4.BeautifulSoup(resp.text, 'lxml')

    movie_items = soup.select('div.item')  # é€‰æ‹©æ¯ä¸ªç”µå½±æ¡ç›®çš„å®¹å™¨

    for item in movie_items:
        title_a = item.select_one('div.info > div.hd > a')  # é€‰æ‹©æ ‡é¢˜
        rank_span = item.select_one('div.info > div.bd > div.star > span.rating_num')  # é€‰æ‹©è¯„åˆ†
        inq = item.select_one('div.bd > p:nth-of-type(1)')  # é€‰æ‹©ç¬¬ä¸€ä¸ªç®€çŸ­ä»‹ç»ï¼Œå¯èƒ½åŒ…å«å¯¼æ¼”å’Œä¸»æ¼”

        if title_a and rank_span:
            movie_info = {
                'title': title_a.text,
                'rating': rank_span.text if rank_span else 'N/A',
                'leader_info': inq.text.strip() if inq else 'N/A'
            }
            print(movie_info)
            movies.append(movie_info)
        # å°†moviesåˆ—è¡¨ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open('è±†ç“£ç”µå½±TOP250.json', 'w', encoding='utf-8') as f:
            json.dump(movies, f, ensure_ascii=False, indent=4)  # indent=4ç”¨äºç¾è§‚çš„æ ¼å¼åŒ–è¾“å‡º

        print("Movies have been saved to movies.json")


# è¯»å–JSONæ–‡ä»¶
with open('è±†ç“£ç”µå½±TOP250.json', 'r', encoding='utf-8') as f:
    movies = json.load(f)

# ç»˜åˆ¶è¯„åˆ†ç›´æ–¹å›¾
ratings = [movie['rating'] for movie in movies]
plt.figure(figsize=(10, 6))
plt.hist(ratings, bins=len(set(ratings)), align='mid', rwidth=0.8, color='skyblue', edgecolor='black')
plt.xlabel('Rating')
plt.ylabel('Frequency')
plt.title('Distribution of Movie Ratings')
plt.xticks(rotation=45)  # å¦‚æœæ ‡ç­¾å¤ªé•¿ï¼Œå¯ä»¥æ—‹è½¬å®ƒä»¬
plt.grid(axis='y', alpha=0.75)
plt.show()

# ç»˜åˆ¶ä¸€ä¸ªæ¡å½¢å›¾æ¥æ˜¾ç¤ºç”µå½±åç§°å’Œè¯„åˆ†
names = [movie['title'].strip().split('\n')[0] for movie in movies]
ratings = [float(movie['rating']) for movie in movies]

# åˆ›å»ºæ¡å½¢å›¾
plt.figure(figsize=(500, 6))
plt.bar(names, ratings)
plt.xlabel('Movie')
plt.ylabel('Rating')
plt.title('Movie Ratings')
plt.xticks(rotation=90)  # æ—‹è½¬xè½´æ ‡ç­¾ä»¥é¿å…é‡å 
plt.show()

# æå–å¹´ä»½åˆ—è¡¨
years = []
for movie in movies:
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹´ä»½
    match = re.search(r'\n\s*(\d{4})\s*[/â€”-]*', movie['leader_info'])
    if match:
        # å¦‚æœåŒ¹é…æˆåŠŸï¼Œæå–å¹´ä»½å¹¶æ·»åŠ åˆ°åˆ—è¡¨ä¸­
        year = int(match.group(1))
        years.append(year)

    # ç»˜åˆ¶å¹´ä»½ç›´æ–¹å›¾
plt.figure(figsize=(10, 6))
plt.hist(years, bins=range(min(years) - 2, max(years) + 3, 1), align='mid', rwidth=0.8, color='skyblue',
         edgecolor='black')
plt.xlabel('Year')
plt.ylabel('Frequency')
plt.title('Distribution of Movie Release Years')
plt.xticks(rotation=45)  # å¦‚æœæ ‡ç­¾å¤ªé•¿ï¼Œå¯ä»¥æ—‹è½¬å®ƒä»¬
plt.grid(axis='y', alpha=0.75)
plt.show()

# æå–æ‰€æœ‰å›½å®¶å¹¶ç»Ÿè®¡æ¬¡æ•°
countries = []
for movie in movies:
    # åŒ¹é…å¹´ä»½åçš„æ‰€æœ‰éç©ºç™½ã€éåˆ†éš”ç¬¦çš„å›½å®¶åç§°
    match = re.search(r'\d{4}\s*[/â€”-]*\s*([^\s/â€”-]+(?:\s+[^\s/â€”-]+)*)', movie['leader_info'])
    if match:
        # åˆ†å‰²åŒ¹é…åˆ°çš„å­—ç¬¦ä¸²ä»¥è·å–æ¯ä¸ªå›½å®¶
        countries.extend(match.group(1).split())

    # ä½¿ç”¨Counterç»Ÿè®¡æ¬¡æ•°
country_counts = Counter(countries)

# ç»˜åˆ¶ç›´æ–¹å›¾
plt.figure(figsize=(10, 6))
plt.bar(country_counts.keys(), country_counts.values(), align='center')
plt.xlabel('Country')
plt.ylabel('Frequency')
plt.title('Country Distribution')
plt.xticks(rotation=45)  # å¦‚æœæ ‡ç­¾å¤ªé•¿ï¼Œå¯ä»¥æ—‹è½¬å®ƒä»¬
plt.tight_layout()  # è‡ªåŠ¨è°ƒæ•´å­å›¾å‚æ•°ï¼Œä½¿ä¹‹å¡«å……æ•´ä¸ªå›¾åƒåŒºåŸŸ
plt.show()

# æå–å¹´ä»½å’Œè¯„åˆ†
years_ratings = []
for movie in movies:
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å¹´ä»½
    match_year = re.search(r'\n\s*(\d{4})\s*[/â€”-]*', movie['leader_info'])
    if match_year:
        year = int(match_year.group(1))
        rating = float(movie['rating'])
        years_ratings.append((year, rating))

    # ç»˜åˆ¶å¹´ä»½ä¸è¯„åˆ†çš„æ•£ç‚¹å›¾
plt.figure(figsize=(10, 6))
years, ratings = zip(*years_ratings)  # è§£å‹å¹´ä»½å’Œè¯„åˆ†
plt.scatter(years, ratings, color='skyblue', edgecolor='black')
plt.xlabel('Year')
plt.ylabel('Rating')
plt.title('Year vs Rating Scatter Plot')
plt.xticks(rotation=45)  # å¦‚æœæ ‡ç­¾å¤ªé•¿ï¼Œå¯ä»¥æ—‹è½¬å®ƒä»¬
plt.grid(axis='y', alpha=0.75)
plt.show()
```

### æ•ˆæœ

- è¯„åˆ†å’Œå¹´ä»£çš„å…³ç³»

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240722-1721651430745)

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240722-1721651438023)

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240722-1721651445084)


### è±†ç“£è¯„è®º

```
import requests
from lxml import etree
#import pandas as pd
#import time

def page(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/116.0',
               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
               'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
               # 'Accept-Encoding': 'gzip, deflate, br',
               'Connection': 'keep-alive',
               'Referer': 'https://movie.douban.com/',
               'Upgrade-Insecure-Requests': '1',
               'Sec-Fetch-Dest': 'iframe',
               'Sec-Fetch-Mode': 'navigate',
               'Sec-Fetch-Site': 'cross-site',
               'Cookie': 'll="118381"; bid=u8W938yW3Rw; __utma=30149280.1427340690.1714406596.1714476685.1714485673.6; __utmz=30149280.1714485673.6.3.utmcsr=baidu|utmccn=(organic)|utmcmd=organic; dbcl2="230302338:Y8MCu494BKI"; push_noty_num=0; push_doumail_num=0; __utmv=30149280.23030; __gads=ID=20dba742ff0011d4:T=1714406703:RT=1714485675:S=ALNI_MZz8jZp982rT48QdQdDBjh5a_vi0w; __gpi=UID=00000dff8c542177:T=1714406703:RT=1714485675:S=ALNI_MZxicDjcSAMi_pdkeduQ3otQCutaQ; __eoi=ID=4874846fe6a3305c:T=1714406703:RT=1714485675:S=AA-Afjb2qzoXLCtivOMEIgch2k1e; __utmc=30149280; ck=mJ0K; frodotk_db="f723232f0ba97d93e569698d22fc78b9"; __utmb=30149280.2.10.1714485673; ap_v=0,6.0; __utmt=1',
               }
    response= requests.get(url=url ,headers=headers)
    c_html = response.content
    return c_html

def next_p(c_html,base_url):
    link = None
    html_n = etree.HTML(c_html)
    lu_next = html_n.xpath('//div[@id="paginator"]/a[@class="next"]/@href')
    if lu_next:
        link = base_url + lu_next[0]
   # time.sleep(5)
    return link

def pin(c_html):
    #print(c_html)
    et_html= etree.HTML(c_html)
    lujing = et_html.xpath("/html/body/div[3]/div[1]/div/div[1]/div[4]/div/div[2]/p/span")
    return lujing
    '''for i in lujing:
        return i.text'''

def tol():
    base_url = 'https://movie.douban.com/subject/4321270/comments'
    link = base_url
    while link:
        c_html = page(link)
        data=[]
        for j in pin(c_html):
            data.append(j.text)
        print(data)
        link=next_p(c_html,base_url)
    return data

if __name__ == '__main__':
    #df = pd.DataFrame(tol())
    #df.to_csv('output.csv',index = False, encoding='utf-8')
    tol()


```

### æ€»ç»“
- è±†ç“£çˆ¬å®Œäº†
- è¿˜èƒ½çˆ¬ç‚¹ä»€ä¹ˆå‘¢?ğŸ¤”
- ä¸‹æ¬¡å†è¯´
