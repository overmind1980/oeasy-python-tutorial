---
show: step
version: 1.0
enable_checker: true
---

# xpath 路径表达式

## 回忆

- 上次真的爬了一个网站
  - oeasy.org
  - 想要得到 菜单上的所有链接
  
1. 获取 xpath
	- 右键检查元素
2. 爬取之后
	- 获得属性 href 的值
	- 然后切片并拼接为绝对链接地址
	- 并且存储到了 urls.txt 中

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20220713-1657722223759)

- 能进入 这些得到的链接
	- 再去 深入爬取 吗？🤔

### 遍历

- 另开
	- 一个o2z.py
	- 然后打开修改

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.readlines()
for url in urls:
    response = requests.get(url)
    b_html = response.content
    et_html = etree.HTML(b_html)
    et_targets = et_html.xpath(".")
    for et_target in et_targets:
        print(et_target.tag)
```

- 运行出现了 12 次 html 节点

### 尝试输出字节流

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.readlines()
for url in urls:
    response = requests.get(url)
    b_html = response.content
    print(b_html)
    et_html = etree.HTML(b_html)
    et_targets = et_html.xpath(".")
    for et_target in et_targets:
        print(et_target.tag)
```

- 结果

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713152199291)

- 爬取到的页面为404

### 输出url

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713152491889)

- 结果

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713152531167)

- 原来是每个网址后面
	- 多了个`\n`

### 修改

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.readlines()
for url in urls:
    url = url.replace("\n","")
    response = requests.get(url)
    b_html = response.content
    print(b_html)
    et_html = etree.HTML(b_html)
    et_targets = et_html.xpath(".")
    for et_target in et_targets:
        print(et_target.tag)
```

- 替换掉回车标记

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713152701913)

- 响应 不再 是404了

### 查找xpath

- 右键单击看电路基础
	- 检查元素

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713152845095)

- 第一个子栏目
	- 爱情 和 其他子栏目不同
	- 从列表中 切去
- 找到电路基础的xpath
	- /html/body/div/div[1]/div[1]

### 修改代码

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.readlines()[1:]
for url in urls:
    url = url.replace("\n","")
    response = requests.get(url)
    b_html = response.content
    et_html = etree.HTML(b_html)
    et_targets = et_html.xpath("/html/body/div/div[1]/div[1]")
    for et_target in et_targets:
        print(et_target.text)
```

- 得到每个子页面的
	- 第一个栏目名

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713152970469)

- 想要得到每个子页面的
	- 所有栏目名呢？

### 修改xpath

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713153084126)

- 查找到每个链接

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713153109104)

- 除了课程名
  - 还想保存教程网址

### 新的xpath

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20220713-1657723049808)

- 找到 和旧版 的关联性
	- 一个题目
	- 对应 一个链接

### 代码

```
import requests
from lxml import etree
with open("urls.txt") as f:
    urls = f.readlines()[1:]
for url in urls:
    url = url.replace("\n","")
    response = requests.get(url)
    b_html = response.content
    et_html = etree.HTML(b_html)
    et_targets = et_html.xpath("/html/body/div/div/div[1]")
    et_links = et_html.xpath("/html/body/div/div/div[2]/a[1]")
    for num in range(len(et_targets)):
        print(et_targets[num].text,end=",")
        print(et_links[num].attrib["href"])
```

- 结果
	- 可以把各个网址输出到屏幕

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713153306791)

- 可以将输出结果
	- 重定向到文本吗?

### 输出重定向

- :w|!python3 % > urls.csv
	- 输出重定向 到一个csv文件

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713153541586)

- 可以理解为
	- 一个文本形式的
	- 电子表格
- 可以用电子表格打开吗？

### 安装电子表格软件

```
sudo apt update
yes|sudo apt install libreoffice

```

- 安装完毕

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713153988405)

### 打开csv

```
libreoffice urls.csv
```

- 打开电子表格

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713154085083)


### 回顾流程

- 我们先爬的是首页
  - 爬出很多链接
  - 然后存到 urls.txt 的里面
- 然后遍历 urls 里面的每一个地址
  - 得到课程的名字和链接
  - 然后入库

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713154167399)

- 可以 将数据 放入
	- 真正的数据库吗？

### 入库

- 更新系统安装 postgres
- 启动 postgres
- 并以 postgres 登录

```
sudo apt update
yes | sudo apt install postgresql
sudo /etc/init.d/postgresql start
sudo -u postgres psql

```

- 查看所有数据库 database

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630664944685)

### 查看数据库

```
\l
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665003642)

- 建立数据库 oeasy
- 并进入

```
CREATE DATABASE 
    oeasy;
\l
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665050881)

- 按<kbd>q</kbd
	- 退出查询

### 进入数据库

```
\c oeasy
\dt
```

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20210903-1630665134696)

###  建表

```
CREATE TABLE urls(
    topic varchar, 
    url varchar
);
\dt
```

- 建表并查看

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713154508667)

###  导入数据

```
\COPY urls  FROM  '/home/shiyanlou/urls.csv'  DELIMITER ',' CSV ;
```

- 导入33条

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713154638061)

### 查询数据

```
SELECT 
    * 
FROM 
    urls
WHERE
    topic LIKE '%e%'
;
```

- 结果

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713154675093)

### 另外的查询

```
SELECT 
    * 
FROM 
    urls
WHERE
	LENGTH(topic) > '5'
;
```

- 筛选结果

![图片描述](https://doc.shiyanlou.com/courses/uid1190679-20240415-1713154703809)


### 总结

- 这次真的爬了一个网站
  - oeasy.org
- 右键检查元素
  - 获取 xpath
- 爬取之后获得属性 href 的值
- 然后切片并拼接为绝对链接地址
- 并且把每一个链接都爬了一遍
- 能出去爬个百度么？🤔
- 下次再说
