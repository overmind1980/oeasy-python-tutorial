```
pip install feedparser
```


- 通过rsshub。app来得到rss信息源

```
import feedparser #导入RSS解析器模块
from wordcloud import WordCloud #导入词云模块
import matplotlib.pyplot as plt #导入绘画模块
import jieba #导入jieba分词模块
from collections import Counter #导入计数模块

#此函数：排除无意义的词汇和自定义重复出现的词汇
def pretty_cut(sentence):
    cut_list = jieba.lcut(sentence, cut_all = False) #使用jieba默认的分词模式，将文本进行分词
    for i in range(len(cut_list)-1, -1, -1): #遍历排除词汇
        if cut_list[i] in stopwords: #如果包含在stopwords里，就删掉那个词汇
            del cut_list[i]
    return cut_list #返回分词词汇

news_url_list=[ #新闻源RSS格式汇总，可以继续增加
"https://rsshub.app/weibo/search/hot",#微博热搜
"https://plink.anyfeeder.com/netease/today",#网易新闻
"https://rsshub.app/zhihu/daily",#知乎日报
"https://rsshub.app/people",#人民网
"https://plink.anyfeeder.com/newscn/whxw",#新华网
"https://plink.anyfeeder.com/thepaper",#澎湃新闻
"https://plink.anyfeeder.com/infzm/news",#南方周末
"https://rsshub.app/chinanews"]#中新网
mystopwords=['中国','要闻','经济','2023','2022','年','月','日'] #自定义排除词汇
stopwords = [i.strip() for i in open('baidu_stopwords.txt',encoding="utf-8").readlines()] #读取百度的排除无意义词汇
stopwords.extend(mystopwords) #无意义词汇加入自定义排除词汇

text="" #定义一个text文本
for i in news_url_list: #新闻源RSS的遍历循环，寻找每个RSS
	d = feedparser.parse(i) #解析得到新闻源的字典
	for i in d.entries: #新闻字典的遍历，寻找每一条新闻内容
		text=text+"\n"+i.title #text加入所有新闻的标题



seg_list=pretty_cut(text) #jieba分词，使用了上面定义的函数
dictionary = Counter(seg_list) #字典统计，分词词频的统计

wordcloud=WordCloud(font_path="WeiRuanYaHei-1.ttf",background_color="white").generate_from_frequencies(dictionary) #设置中文字体：微软雅黑，背景白色，生成词云
plt.figure() #词云图绘制
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show() #词云图展示

```