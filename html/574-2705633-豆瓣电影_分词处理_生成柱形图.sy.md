---
show: step
version: 1.0
enable_checker: true
---

# çˆ¬å–å›¾ç‰‡æ•°æ®

## å›å¿†

- ä¸Šæ¬¡çˆ¬äº†æ±‰å­—æº
- å›¾ç‰‡ä¸åªæ˜¯å¯ä»¥å•ç‹¬å­˜åœ¨
- ä¹Ÿå¯ä»¥åµŒå…¥åˆ°ç½‘é¡µä¸­çš„
- åº”è¯¥æ³¨æ„åˆ°è¯·æ±‚å¤´é‡Œé¢å¯ä»¥æ”¾ä¸œè¥¿
- çˆ¬å–çš„æ—¶å€™å¯ä»¥åˆ©ç”¨åŸæ¥ç½‘é¡µæ•´ä½“çš„ç»“æ„

### è§‚å¯Ÿè±†ç“£ç”µå½±250

```
firefox http://movie.douban.com/top250
```

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717202833221)

### åˆ†é¡µçˆ¬

```
# å¯¼åŒ…
import requests
from lxml import etree      # ç”¨lxmlè§£æå™¨ç”Ÿæˆçš„å¯¹è±¡ä¸­çš„xpathæ–¹æ³•
from time import sleep
import csv
import numpy as np


# æŒ‡å®šURL
# url = 'https://movie.douban.com/top250'
# è¿›è¡ŒUAä¼ªè£…
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36 Edg/117.0.2045.41'}

# å®šä¹‰ç©ºåˆ—è¡¨å­˜æ”¾ç”µå½±æ•°æ®
tiltes_cn = []       # ä¸­æ–‡æ ‡é¢˜
titles_en = []       # è‹±æ–‡æ ‡é¢˜
links = []           # è¯¦æƒ…é¡µé“¾æ¥
director = []        # å¯¼æ¼”
actors = []          # æ¼”å‘˜
years = []           # ä¸Šæ˜ å¹´ä»½
nations = []         # å›½å®¶å’Œåœ°åŒº
types = []           # ç±»å‹
scores = []           # è¯„åˆ†
rating_nums = []      # è¯„åˆ†äººæ•°

fp = open('douban_top250.csv','w',encoding='utf-8')
writer = csv.writer(fp)
writer.writerow(['ç”µå½±ä¸­æ–‡å', 'ç”µå½±è‹±æ–‡å','ç”µå½±è¯¦æƒ…é¡µé“¾æ¥','å¯¼æ¼”','æ¼”å‘˜','ä¸Šæ˜ å¹´ä»½','å›½å®¶å’Œåœ°åŒº','ç±»å‹','è¯„åˆ†','è¯„åˆ†äººæ•°'])

for i in range(0,226,25):
    url = f'https://movie.douban.com/top250?start={i}&filter='
    # å°†URLä¸­çš„å‚æ•°å°è£…åˆ°å­—å…¸ä¸­
    # data = {
    #     'start':i,            # è®¾ç½®startå‚æ•°
    #     'filter':'',
    # }
    # å‘èµ·è¯·æ±‚ï¼Œè·å–ç½‘é¡µå“åº”
    response = requests.get(url,headers=headers
                            # ,data=data
                           )
    sleep(1)
    # print(response.status_code)
    # print(response.encoding)
    # print(response.text)
    
    # è·å–å“åº”å†…å®¹
    html = response.text  
    # å®ä¾‹åŒ–ä¸€ä¸ªetreeå¯¹è±¡
    data = etree.HTML(html)
    
    # æ‰€æœ‰ç”µå½±ä¿¡æ¯éƒ½åœ¨liæ ‡ç­¾ä¸‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥å…ˆå®šä½åˆ°liæ ‡ç­¾ï¼Œåœ¨é€šè¿‡å¾ªç¯è·å–æ¯ä¸€ä¸ªliæ ‡ç­¾ä¸­çš„ä¿¡æ¯
    li_list = data.xpath('//*[@id="content"]/div/div[1]/ol/li')
    # é€šè¿‡å¾ªç¯éå†æ¯ä¸€é¡µä¸­çš„æ‰€æœ‰liæ ‡ç­¾ï¼Œè·å–è¯¥é¡µé¢æ‰€æœ‰ç”µå½±çš„æ•°æ®
    for each in li_list:
        # ä¸­æ–‡æ ‡é¢˜
        title1 = each.xpath('./div/div[2]/div[1]/a/span[1]/text()')[0] 
        tiltes_cn.append(title1)
        
        # è‹±æ–‡æ ‡é¢˜
        # æ¯æ¬¡è·å–åˆ°çš„æ•°æ®å­˜åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œé€šè¿‡ä¸‹æ ‡ç´¢å¼•å–åˆ—è¡¨çš„å€¼
        # é€šè¿‡å­—ç¬¦ä¸²çš„strip()æ–¹æ³•å»é™¤å­—ç¬¦ä¸²é¦–å°¾çš„æŒ‡å®šå­—ç¬¦ä¸²
        title2 = each.xpath('./div/div[2]/div[1]/a/span[2]/text()')[0].strip('\xa0/\xa0')
        titles_en.append(title2)
        
        # é“¾æ¥
        link = each.xpath('./div/div[2]/div[1]/a/@href')[0]     
        links.append(link)
        
        # å¯¼æ¼”ã€ä¸»æ¼”
        info1 = each.xpath('./div/div[2]/div[2]/p[1]/text()[1]')[0].strip()       # é€šè¿‡stripæ–¹æ³•å»é™¤å­—ç¬¦ä¸²çš„å‰åç©ºæ ¼
        split_info1 = info1.split('\xa0\xa0\xa0')      # é€šè¿‡æŒ‡å®šå­—ç¬¦ä¸²åˆ†å‰²å­—ç¬¦ä¸²
        dirt = split_info1[0].strip('å¯¼æ¼”: ')
        director.append(dirt)
        # æœ‰äº›ç”µå½±çš„ä¸»æ¼”ä¸ºç©ºï¼Œæ‰€ä»¥éœ€è¦è¿›è¡Œæ¡ä»¶åˆ¤æ–­
        # å¦‚æœå¯¼æ¼”å’Œä¸»æ¼”ä¿¡æ¯éƒ½æœ‰ï¼Œåˆ™è·å–ä¸»æ¼”ä¿¡æ¯
        if len(split_info1) == 2:
            ac = split_info1[1].strip('ä¸»æ¼”: ')
            actors.append(ac)
        # å¦‚æœæ²¡æœ‰ä¸»æ¼”ä¿¡æ¯ï¼Œåˆ™å°†å…¶ä¿¡æ¯è®¾ç½®ä¸ºç©º
        else:
            actors.append(np.nan)
        
        # å¹´ä»½ã€å›½ç±ã€ç±»å‹
        info2 = each.xpath('./div/div[2]/div[2]/p[1]/text()[2]')[0].strip()    # å»é™¤å­—ç¬¦ä¸²é¦–å°¾çš„ç©ºæ ¼
        split_info2 = info2.split('\xa0/\xa0')    # é€šè¿‡å­—ç¬¦ä¸²åˆ†å‰²è·å–å­—ç¬¦ä¸²ä¸­çš„å¹´ä»½ã€å›½ç±å’Œç±»å‹
        # print(split_info)
        year = split_info2[0]
        nation = split_info2[1]
        ftype = split_info2[2]
        years.append(year)
        nations.append(nation)
        types.append(ftype)
        
        # ç”µå½±è¯„åˆ†
        score = each.xpath('./div/div[2]/div[2]/div/span[2]/text()')[0]
        scores.append(score)
        
        # è·å–ç”µå½±æ‰“åˆ†äººæ•°
        num = each.xpath('./div/div[2]/div[2]/div/span[4]/text()')[0].strip('äººè¯„ä»·')
        rating_nums.append(num)
        
        writer.writerow([title1,title2,link,dirt,ac,year,nation,ftype,score,num])
    print(f'â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”ç¬¬{int((i/25)+1)}é¡µçˆ¬å–å®Œæ¯•ï¼â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”')
        
        
    
fp.close()   # å†™å…¥å®Œæˆåï¼Œå…³é—­æ–‡ä»¶
print('â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”çˆ¬è™«ç»“æŸï¼ï¼ï¼ï¼ï¼â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”')
```

### è§‚å¯Ÿç»“æœ

```
sudo apt install csvtool
```

- å®‰è£…åè§‚å¯Ÿå›½åˆ«

```
csvtool col 1,7 douban_top250.csv
```

- æ•ˆæœ

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717202931071)

- è¯„åˆ†è°æœ€é«˜?è°æœ€ä½?

### è¯„åˆ† 

```
import pandas as pd

df = pd.read_csv('douban_top250.csv', sep=",")

score = df['è¯„åˆ†'].max()
print("æœ€é«˜è¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†'] == score]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\tåˆ†æ•°æ˜¯ï¼š", score)
score = df['è¯„åˆ†'].min()
print("æœ€ä½è¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†'] == score]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\tåˆ†æ•°æ˜¯ï¼š", score)
```

- è¯„åˆ†é«˜ä½

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717223980376)

- è¯„åˆ†äººæ•°
	- è°é«˜è°ä½?

### è¯„åˆ†äººæ•°

```
import pandas as pd

df = pd.read_csv('douban_top250.csv', sep=",")

num = df['è¯„åˆ†äººæ•°'].max()
print("æœ€å¤šäººè¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†äººæ•°'] == num]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\täººæ•°æ˜¯ï¼š", num)
num = df['è¯„åˆ†äººæ•°'].min()
print("æœ€å°‘äººè¯„åˆ†çš„ç”µå½±æ˜¯ï¼š", df[df['è¯„åˆ†äººæ•°'] == num]['ç”µå½±ä¸­æ–‡å'].iloc[0], "\täººæ•°æ˜¯ï¼š", num)
```

- è¯„åˆ†äººæ•°

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717224016678)

- å„ç§ç±»å‹
	- è°å¤šè°å°‘?


### å®‰è£…ç±»åº“

- å®‰è£…è¯äº‘å’Œåˆ†è¯åŒ…

```
pip3 install wordcloud
pip3 install jieba
```

### ç±»å‹

```
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
import csv

def segment(text):
    for ch in '!"#$%&()*+,-./:;<=>?@[\\]^_â€˜{|}~ï¼Œã€‚ï¼Ÿï¼ï¼šï¼›â€â€œâ€™â€˜-=ã€Šã€‹ï¼ˆï¼‰ã€ã€ã€‘â†“LO':
        text = text.replace(ch, " ")
    words_list_jieba = jieba.lcut(text)
    words_list = []
    for i in words_list_jieba:
        words_list.append(i)
    return words_list

def words_frequency(words_list):
    words_dict = {}
    for key in words_list:
        words_dict[key] = words_dict.get(key, 0) + 1
    return words_dict

def words_dict_sort(words_dict, descending):
    temp_dict = sorted(words_dict.items(), key=lambda x: x[1], reverse=descending)
    return temp_dict

df = pd.read_csv('douban_top250.csv', sep=",")
text = ' '.join(df['ç±»å‹'].dropna())
words_list = segment(text)
words_dict = words_frequency(words_list)
print(words_dict)
```

1. å°†æ ‡ç‚¹æ›¿æ¢ä¸ºç©ºæ ¼
2. å°†å­—ç¬¦ä¸²åˆ†è¯ç”Ÿæˆè¯åˆ—è¡¨
3. å°†è¯åˆ—è¡¨ç”Ÿæˆè¯é¢‘å­—å…¸
4. å°†è¯é¢‘å­—å…¸è¾“å‡º

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717224399325)

### ç»“æœ

- ç»“æœå¯ä»¥ç”Ÿæˆ
	- æŸ±é¥¼æŠ˜
	- è¯äº‘å›¾

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717224451129)

- æƒ³è¦å¾—åˆ° 
	- å…³äºå›½å®¶å’Œåœ°åŒºçš„
	- è¯é¢‘å­—å…¸

### æ•°æ®æ¸…æ´—

- å¦‚æœæƒ³è¦å°†ä¸€äº›è¯æ±‡
	- ä»ç»“æœä¸­æ¸…æ´—å‘¢?

```
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import jieba
import csv

def segment(text):
    for ch in '!"#$%&()*+,-./:;<=>?@[\\]^_â€˜{|}~ï¼Œã€‚ï¼Ÿï¼ï¼šï¼›â€â€œâ€™â€˜-=ã€Šã€‹ï¼ˆï¼‰\nã€ã€‘â†“LO':
        text = text.replace(ch, " ")
    words_list_jieba = jieba.lcut(text)
    words_list = []
    for i in words_list_jieba:
        if i in ["ä½ ", "UNG", "å¥¹", "ä¸‰å¨˜", "è¿™ä½", "å¾®åš", "è§†è§’", "0", "ç§€è‹±", "å¾ˆ", "çƒ­è¯", "ä¸¤ä¼š", "çœ‹","è¿™äº›", "èŠ", "æˆ‘ä»¬", "å…³äº", "è¦", "è§†é¢‘", "è¿™", "é‚£", "æœ‰", "ä¸", "åœ¨", "å´", "éƒ½","å°±", "æ˜¯", "å°±æ˜¯", "æˆ‘", "ä»–", "å®ƒ", "ä»–ä»¬","å®ƒä»¬", "çš„", "ç€", "äº†", "å’Œ", "ä¸", "ä¸ª", "ä¸€ä¸ª", "ä¸€","ã€Š", "ã€‹", " ", "\n"] or i.isnumeric():
            pass
        else:
            words_list.append(i)
    return words_list

def words_frequency(words_list):
    words_dict = {}
    for key in words_list:
        words_dict[key] = words_dict.get(key, 0) + 1
    return words_dict

def words_dict_sort(words_dict, descending):
    temp_dict = sorted(words_dict.items(), key=lambda x: x[1], reverse=descending)
    return temp_dict

df = pd.read_csv('douban_top250.csv', sep=",")

text = ' '.join(df['å›½å®¶å’Œåœ°åŒº'].dropna())
words_list = segment(text)
words_dict = words_frequency(words_list)
wordcloud = WordCloud(font_path="/usr/share/fonts/truetype/wqy-zenhei.ttc",  # ä¸ºæ˜¾ç¤ºæ­£ç¡®çš„ä¸­æ–‡ï¼Œéœ€æŒ‡å®šå­—ä½“
                      background_color="white",  # èƒŒæ™¯é¢œè‰²
                      collocations=False,
                      width=800,
                      height=600,
                      margin=2,  # è®¾ç½®å›¾ç‰‡é»˜è®¤çš„å¤§å°
                      ).fit_words(words_dict)
wordcloud.to_file('nation.png')

```

- å¯ä»¥æŠŠæŸäº›å•è¯
	- æ’é™¤

### æœ€ç»ˆç»“æœ

- é¦–å…ˆå£°æ˜
	- é¦™æ¸¯åœ°åŒºã€æ¾³é—¨åœ°åŒºã€å°æ¹¾åœ°åŒºæ˜¯
	- ä¸­å›½ä¸å¯åˆ†å‰²çš„ä¸€éƒ¨åˆ†

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717225094789)

- å¦‚æœæƒ³è¦å°†
	- ä¸­å›½å¤§é™†ã€ä¸­å›½é¦™æ¸¯ã€ä¸­å›½å°æ¹¾
	- ä½œä¸ºåœ°åŒº åˆ†ç±»ç»Ÿè®¡

### æ·»åŠ è¯æ±‡

- å°†ä¸‰ä¸ªå•è¯åŠ å…¥å­—å…¸è¯æ¡

```
jieba.add_word("ä¸­å›½å¤§é™†", freq=None, tag=None)
jieba.add_word("ä¸­å›½å°æ¹¾", freq=None, tag=None)
jieba.add_word("ä¸­å›½é¦™æ¸¯", freq=None, tag=None)
```

- ç»“æœ

![å›¾ç‰‡æè¿°](https://doc.shiyanlou.com/courses/uid1190679-20240601-1717225450110)

- æœŸå¾…ç¥–å›½ç¬¬å…«è‰ºæœ¯å’Œç¬¬ä¹è‰ºæœ¯èµ·é£

### æ€»ç»“
- è±†ç“£çˆ¬å®Œäº†
- è¿˜èƒ½çˆ¬ç‚¹ä»€ä¹ˆå‘¢?ğŸ¤”
- ä¸‹æ¬¡å†è¯´
